{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name: Sanzida Parvin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install the tweepy package before using it!\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save consumer key, consumer secret, access token, and access secret\n",
    "con_key = 'con_key'\n",
    "con_secret = 'con_secret'\n",
    "acc_token = 'acc_token'\n",
    "acc_secret = 'acc_secret'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use tweepy.OAuthHandler to create an authentication using the given key and secret\n",
    "auth = tweepy.OAuthHandler(consumer_key=con_key, consumer_secret=con_secret)\n",
    "auth.set_access_token(acc_token, acc_secret)\n",
    "\n",
    "# Connect to the Twitter API using the authentication\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ************ Tweet Collection For Alaska Airline **************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect tweets for more than 100 at a time\n",
    "num_needed = 1000 \n",
    "tweet_list_Alaska = []\n",
    "last_id = -1 # id of last tweet seen\n",
    "\n",
    "while len(tweet_list_Alaska) < num_needed:\n",
    "    try:\n",
    "        #Use the REST API for a static search\n",
    "        new_tweets_Alaska = api.search(q = 'AlaskaAir OR \"Alaska Airlines\" AND -filter:media AND -filter:retweets AND -filter:replies AND -filter:links', \n",
    "                                       since = \"2017-12-18\", until = \"2017-12-20\", count = 1000, lang='en', max_id = str(last_id - 1), tweet_mode=\"extended\")\n",
    "    except tweepy.TweepError as e:\n",
    "        print(\"Error\", e)\n",
    "        break\n",
    "    else:\n",
    "        if not new_tweets_Alaska:\n",
    "            print(\"Could not find any more tweets!\")\n",
    "            break\n",
    "        tweet_list_Alaska.extend(new_tweets_Alaska)  \n",
    "        \n",
    "        # get the full length of the tweet\n",
    "        tweets_Alaska = [[tweet.full_text] for tweet in tweet_list_Alaska]\n",
    "        last_id = new_tweets_Alaska[-1].id\n",
    "\n",
    "len(tweet_list_Alaska)\n",
    "#tweet_list_Alaska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pandas, numpy and csv packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save tweets as data frame to create csv file\n",
    "# Create DataFrame with columns Tweet_Text, Tweet_ID, Date, and Likes_Count\n",
    "\n",
    "data_Alaska = pd.DataFrame(data = tweets_Alaska, columns = [\"Tweet_Text\"]) \n",
    "data_Alaska['Tweet_ID'] = np.array([tweet.id for tweet in tweet_list_Alaska])\n",
    "data_Alaska['Date'] =np.array([tweet.created_at for tweet in tweet_list_Alaska])\n",
    "data_Alaska['Likes_Count'] = np.array([tweet.favorite_count for tweet in tweet_list_Alaska])\n",
    "Alaska_noduplicates = data_Alaska.drop_duplicates(subset = \"Tweet_ID\") # Remove duplicates\n",
    "#Alaska_noduplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the collected tweets in a csv file for the first time\n",
    "# Alaska_noduplicates.to_csv('Alaska.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the collected tweets in a csv file for next times\n",
    "Alaska_noduplicates.to_csv('Alaska2.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read both of the files to append and get a full dataset\n",
    "\n",
    "Alaska = pd.read_csv('C:/Users/...../Alaska.csv', encoding = \"ISO-8859-1\")\n",
    "Alaska2 = pd.read_csv('C:/Users/...../Alaska2.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# append the tweet search for different days and make one file\n",
    "Alaska = Alaska.append(Alaska2, ignore_index=True)\n",
    "#Alaska\n",
    "len(Alaska)\n",
    "\n",
    "# Remove duplicates from full data_frame to get the final DataFrame without any duplicates\n",
    "Alaska_final = Alaska.drop_duplicates(subset = \"Tweet_ID\") \n",
    "len(Alaska_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the full dataframe in one csv file\n",
    "Alaska_final.to_csv('Alaska.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import textblob package for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Referance of applying textblob: https://dev.to/rodolfoferro/sentiment-analysis-on-trumpss-tweets-using-python-\n",
    "\n",
    "# function to clean the text in a tweet (removing links and special characters)\n",
    "def clean_tweet(tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "# function to classify the polarity (Like, Neutral, Dislike) of a tweet using textblob\n",
    "def analize_sentiment(tweet):\n",
    "    analysis = TextBlob(clean_tweet(tweet)) # clean the tweet inside the function\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return \"Like\"\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Dislike\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the uncleaned csv file for Alaska\n",
    "Alaska_df = pd.read_csv('C:/Users/...../Alaska.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# create a column with the expression of the sentiment analysis after cleaning the text\n",
    "Alaska_df['Sentiment'] = np.array([analize_sentiment(tweet) for tweet in Alaska_df['Tweet text']])\n",
    "Alaska_df\n",
    "\n",
    "# Remove unnessary columns\n",
    "Alaska_SA = Alaska_df.drop(['Tweet_text','Date','Likes_Count'], axis = 1) \n",
    "len(Alaska_SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the final data frame with sentiment analysis to a csv file to use in R \n",
    "Alaska_SA.to_csv('Alaska_SA.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ************ Tweet Collection For JetBlue Airline **************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect tweets for more than 100 at a time\n",
    "num_needed = 1000\n",
    "tweet_list_JetBlue = []\n",
    "last_id = -1 # id of last tweet seen\n",
    "while len(tweet_list_JetBlue) < num_needed:\n",
    "    try:\n",
    "        # Use the REST API for a static search\n",
    "        new_tweets_JetBlue = api.search(q = 'JetBlue OR JetBlueAir OR \"JetBlue Airline\" AND -filter:media AND -filter:retweets AND -filter:replies AND -filter:links', \n",
    "                                        since = \"2017-12-15\", until = \"2017-12-19\", count = 1000, lang='en', max_id = str(last_id - 1), tweet_mode=\"extended\")\n",
    "    except tweepy.TweepError as e:\n",
    "        print(\"Error\", e)\n",
    "        break\n",
    "    else:\n",
    "        if not new_tweets_JetBlue:\n",
    "            print(\"Could not find any more tweets!\")\n",
    "            break\n",
    "        tweet_list_JetBlue.extend(new_tweets_JetBlue) # get more than 100 tweets \n",
    "        # get the full length of the tweet text\n",
    "        tweets_JetBlue = [[tweet.full_text] for tweet in tweet_list_JetBlue]\n",
    "        last_id = new_tweets_JetBlue[-1].id\n",
    "len(tweets_JetBlue)\n",
    "#tweet_list_JetBlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save tweets as data frame to create csv file\n",
    "# Create DataFrame with columns Tweet_Text, Tweet_ID, Date, and Likes_Count\n",
    "\n",
    "data_JetBlue = pd.DataFrame(data = tweets_JetBlue, columns = [\"Tweet_text\"])\n",
    "data_JetBlue['Tweet_ID'] = np.array([tweet.id for tweet in tweet_list_JetBlue])\n",
    "data_JetBlue['Date'] =np.array([tweet.created_at for tweet in tweet_list_JetBlue])\n",
    "data_JetBlue['Likes_Count'] = np.array([tweet.favorite_count for tweet in tweet_list_JetBlue])\n",
    "JetBlue_noduplicates = data_JetBlue.drop_duplicates(subset = \"Tweet_ID\") # Remove duplicates\n",
    "len(JetBlue_noduplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save sentiment analysis to a csv file for the first time \n",
    "# JetBlue_noduplicates.to_csv('JetBlue.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the collected tweets in a csv file for the next times\n",
    "JetBlue_noduplicates.to_csv('JetBlue2.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read both of the files to append and get a full dataset\n",
    "\n",
    "JetBlue = pd.read_csv('JetBlue.csv', encoding = \"ISO-8859-1\")\n",
    "JetBlue2 = pd.read_csv('JetBlue2.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# append the tweet search for different days and make one file\n",
    "JetBlue = JetBlue.append(JetBlue2, ignore_index=True)\n",
    "len(JetBlue)\n",
    "\n",
    "JetBlue_final = JetBlue.drop_duplicates(subset = \"Tweet_ID\") # Remove duplicates\n",
    "len(JetBlue_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save full data in one csv file\n",
    "JetBlue_final.to_csv('JetBlue.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the uncleaned csv file\n",
    "JetBlue_df = pd.read_csv('C:/Users/...../JetBlue.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# create a column with the expression of the sentiment analysis after cleaning the text\n",
    "JetBlue_df['Sentiment'] = np.array([analize_sentiment(tweet) for tweet in JetBlue_df['Tweet_text']])\n",
    "len(JetBlue_df)\n",
    "\n",
    "# Remove unnessary columns\n",
    "JetBlue_SA = JetBlue_df.drop(['Tweet_text','Date','Likes_Count'], axis = 1) \n",
    "len(JetBlue_SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save sentiment analysis to a csv file to open it in R \n",
    "JetBlue_SA.to_csv('JetBlue_SA.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ************ Tweet Collection For Southwest Airline **************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect tweets for more than 100 at a time\n",
    "num_needed = 1000\n",
    "tweet_list_Southwest = []\n",
    "last_id = -1 # id of last tweet seen\n",
    "while len(tweet_list_Southwest) < num_needed:\n",
    "    try:\n",
    "        # Use the REST API for a static search\n",
    "        new_tweets_Southwest = api.search(q = 'SouthwestAir OR \"Southwest Airlines\" AND -filter:media AND -filter:retweets AND -filter:replies AND -filter:links', \n",
    "                                         since = \"2017-12-15\", until = \"2017-12-19\", lang='en', count = 1000, max_id = str(last_id - 1), tweet_mode=\"extended\")\n",
    "    except tweepy.TweepError as e:\n",
    "        print(\"Error\", e)\n",
    "        break\n",
    "    else:\n",
    "        if not new_tweets_Southwest:\n",
    "            print(\"Could not find any more tweets!\")\n",
    "            break\n",
    "        tweet_list_Southwest.extend(new_tweets_Southwest) # get more than 100 tweets \n",
    "        # get the full length of the tweet text\n",
    "        tweets_Southwest = [[tweet.full_text] for tweet in tweet_list_Southwest]\n",
    "        last_id = new_tweets_Southwest[-1].id\n",
    "len(tweet_list_Southwest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save tweets as data frame to create csv file\n",
    "# Create DataFrame with columns Tweet_Text, Tweet_ID, Date, and Likes_Count\n",
    "\n",
    "data_Southwest = pd.DataFrame(data = tweets_Southwest, columns = [\"Tweet_text\"])\n",
    "data_Southwest['tweet_ID'] = np.array([tweet.id for tweet in tweet_list_Southwest])\n",
    "data_Southwest['Date'] =np.array([tweet.created_at for tweet in tweet_list_Southwest])\n",
    "data_Southwest['Likes_Count'] = np.array([tweet.favorite_count for tweet in tweet_list_Southwest])\n",
    "Southwest_noduplicates = data_Southwest.drop_duplicates(subset = \"tweet_ID\") # Remove duplicates\n",
    "len(Southwest_noduplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save sentiment analysis to a csv file  \n",
    "# Southwest_noduplicates.to_csv('Southwest.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the collected tweets in a csv file for the next times\n",
    "Southwest_noduplicates.to_csv('Southwest2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read both of the files to append and get a full dataset\n",
    "\n",
    "Southwest = pd.read_csv('Southwest.csv', encoding = \"ISO-8859-1\")\n",
    "Southwest2 = pd.read_csv('Southwest2.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# append the tweet search for different days and make one file\n",
    "Southwest = Southwest.append(Sothwest2, ignore_index=True)\n",
    "len(Southwest)\n",
    "\n",
    "Southwest_final = Southwest.drop_duplicates(subset = \"tweet_ID\") # Remove duplicates\n",
    "len(Southwest_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save full data in one csv file\n",
    "Southwest_final.to_csv('Southwest.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the uncleaned csv file\n",
    "Southwest_df = pd.read_csv('C:/Users/...../Southwest.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# create a column with the expression of the sentiment analysis after cleaning the text\n",
    "Southwest_df['Sentiment'] = np.array([analize_sentiment(tweet) for tweet in Southwest_df['Tweet_text']])\n",
    "len(Southwest_df)\n",
    "\n",
    "# Remove unnessary columns\n",
    "Southwest_SA = Southwest_df.drop(['Tweet_text','Date','Likes_Count'], axis = 1) \n",
    "len(Southwest_SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save sentiment analysis to a csv file to open in R\n",
    "Southwest_SA.to_csv('Southwest_SA.csv', index = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
